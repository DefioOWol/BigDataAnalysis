# Белая книга проекта BigData-аналитики

## Концепция

**Проект:** "Мониторинг и анализ цен на товары"  
**Цель:** сбор цен / наличия / акций, подготовка витрин и аналитика динамики цен
(по категориям, выявление аномалий, прогноз), с итоговыми документами для условного "бизнеса".


## Предполагаемое описание

### Источники данных, хранилища и технологии хранения

**Источники данных**
- Файлы с ценами и товарами в формате *CSV* или *JSON*
    (поля: `дата`, `магазин`, `товар`, `категория`, `цена`, `наличие`, `акция`);
- Данные из API: выгрузка цен и наличия раз в сутки.

**Хранилища и технологии хранения**
- Hadoop - основное хранилище данных проекта (исходные, очищенные, приведенные, витрины);
- Hive - хранение данных в виде таблиц поверх HDFS и выполнение SQL‑запросов;
- Для обработанных данных можно использовать **Parquet**;
    исходные данные сохраняются в *CSV* / *JSON*.


---
### Методы и средства сбора, предобработки и отображения

**Сбор данных**
- Периодичность: раз в сутки;
- Средства: Python‑скрипты, которые получают данные из файлов / API и загружают их в HDFS;
- Контроль выполнения: запись времени загрузки, количества строк и статуса выполнения.

**Предобработка**
- Приведение типов;
- Удаление пустых строк и дублирований;
- Нормализация написания категорий и названий;
- Фильтрация ошибочных значений.

**Отображение**
- Дашборды, например, в **Apache Superset**;
- Набор графиков:
    - динамика средней цены;
    - сравнение магазинов;
    - список товаров с самым большим ростом / падением цен;
    - отметки аномалий.


---
### Технологии, методы и средства обработки данных

Обработка выполняется в экосистеме Hadoop.
- хранение: HDFS;
- обработка и запросы: Hive (SQL).

Этапы обработки:
- загрузка исходных данных в **raw** слой;
- формирование **clean** слоя для очистки и предобработки;
- расчет витрин в **mart** слое:
    - цены по дням и категориям;
    - цены по магазинам;
    - изменения цены относительно предыдущего дня / недели.


---
### Технологии и методы анализа, итоговые документы

**Методы анализа**
- Статистика: средняя цена, медиана, минимум / максимум по категории;
- Сравнение периодов: насколько цены выросли / упали за неделю / месяц;
- Поиск аномалий: резкий скачок цены за день, сильное отклонение от типичных значений.

**Итоговые документы**
- Описание источников данных;
- Схема хранения в HDFS;
- Краткое описание обработки;
- Витрины и примеры запросов (Hive SQL);
- Дашборды со скриншотами и выводами.


---
### Реестр заинтересованных лиц и способы их информирования

- **Заказчик (менеджер / маркетолог)**  
    **Что нужно**: понять динамику цен, видеть скачки и сравнение магазинов.  
    **Информирование**: короткое демо, какие вопросы решаются с накоплением
        истории цен в HDFS и расчетами в Hive.  
    **Собеседование**: уточнение целей, периодов сравнения, требований к отчетам / дашбордам.  
    **Интервью**: согласование списка показателей, перечня категорий, формата представления.

- **Аналитик данных**  
    **Что нужно**: доступ к чистым данным и понятные определения показателей.  
    **Информирование**: пояснение, какие таблицы доступны в Hive, какие витрины формируются,
        какие запросы можно выполнять.  
    **Собеседование**: согласование определений показателей, правил расчета.  
    **Интервью**: обсуждение структуры таблиц, примеров запросов.

- **Инженер данных**  
    **Что нужно**: простой и стабильный процесс загрузки и обработки данных.  
    **Информирование**: описание пайплайна, точек контроля.  
    **Собеседование**: согласование расписания запуска, структуры папок в HDFS,
        правил хранения данных.  
    **Интервью**: определение требований к качеству данных.

- **Администратор / DevOps**  
    **Что нужно**: как развернуть Hadoop и сопутствующие инструменты, какие ресурсы нужны.  
    **Информирование**: краткое описание компонентов, их роли.  
    **Собеседование**: согласование способа развертывания, ресурсов.  
    **Интервью**: уточнение ограничений среды.
